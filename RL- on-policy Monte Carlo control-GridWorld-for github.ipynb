{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a9125a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Monte Carlo methods: On-policy Monte Carlo control\n",
    "\n",
    "A **Monte Carlo** (**MC**) method is a model-free method which is used in **Reinforcement Learning** (RL). Its difference wuth **temporal difference** (TD) methods is that MC methods do not use *bootstrapping* and they are **episode-driven**, which means they update value functions and policies after an episode terminates. \n",
    "<br> An MC control is the MC method in which both **policy evaluation** and **policy improvement** are performed. **On-policy** for an MC control means that the **behavier policy** and the **target policy** are the same.\n",
    "<br>**Hint:** *Policy evaluation* estimates the value function for the current policy using MC sampling. In contrast, *policy improvement* greedily updates the policy to choose actions with the highest estimated values.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the example in this Notebook, we use the same **Grid World** environment (similar to the one we used before):\n",
    " - **States:** A sizexsize grid (size*size states), labeled as (0,0) to (size-1,size-1).\n",
    " - **Actions:** Up, Down, Left, Right.\n",
    " - **Rewards:**\n",
    "    - Reaching the goal state (size-1,size-1) gives a reward of +10.\n",
    "    - Reaching a \"pit\" state (size//2,size//2) gives a reward of −10.\n",
    "    - All other transitions give a reward of −1.\n",
    "- **Terminal States:** (size-1,size-1) (goal) and (size//2,size//2) (pit).\n",
    "- **Transition Probabilities:**\n",
    "    - Moving in the intended direction succeeds with probability 0.8.\n",
    "    - With probability 0.2, the agent moves in a random direction\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the following, we first implement the GridWrold. Then, we implement on-policy MC control both for **first-visit** and **every-visit** variants.\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405ab30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d082d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the GridWorld environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.terminal = {(size-1, size-1):10}  # Goal at bottom-right\n",
    "        self.pits = {(size//2, size//2):-10}   # Pit at center\n",
    "        self.current_state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = (0, 0)  # Start at top-left\n",
    "        return self.current_state\n",
    "\n",
    "    def _move(self, action_idx, i, j):\n",
    "        action=self.actions[action_idx]\n",
    "        if action == 'up': return (max(i-1, 0), j)\n",
    "        elif action == 'down': return (min(i+1, self.size-1), j)\n",
    "        elif action == 'left': return (i, max(j-1, 0))\n",
    "        elif action == 'right': return (i, min(j+1, self.size-1))\n",
    "        \n",
    "    def step(self, action_idx):\n",
    "        i, j = self.current_state\n",
    "        if self.current_state in self.terminal or self.current_state in self.pits:\n",
    "            return self.current_state, 0, True\n",
    "        \n",
    "        # Movement with stochasticity\n",
    "        if random.random() < 0.8:\n",
    "            next_state = self._move(action_idx, i, j)\n",
    "        else:\n",
    "            next_state = self._move(random.choice(range(len(self.actions))), i, j)\n",
    "        \n",
    "        self.current_state =next_state\n",
    "\n",
    "        reward = self.terminal.get(next_state, self.pits.get(next_state, -1))\n",
    "        done = next_state in self.terminal or next_state in self.pits\n",
    "        return self.current_state, reward, done  # Step penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85cc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every-visit or first-visit on-policy Monte Carlo control for the GridWorld\n",
    "# Choose FRIST_VISIT=True to have first-visit MC control\n",
    "# otherwise,it performs every-visit MC control\n",
    "def monte_carlo_q_learning(env, episodes=10000, gamma=0.9, epsilon=0.3,FIRST_VISIT=False):\n",
    "    # Initialize Q-table: maps (i,j) → [Q(s,a₁), Q(s,a₂), ...]\n",
    "    Q = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "    returns_count = defaultdict(lambda: np.zeros(len(env.actions)))\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        # Generate episode\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # ε-greedy policy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(range(len(env.actions)))\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        # if you want first-visit, do these\n",
    "        if FIRST_VISIT:\n",
    "            first_occurrences = {}\n",
    "            for t, (s, a, _) in enumerate(episode):\n",
    "                if (s, a) not in first_occurrences:\n",
    "                    first_occurrences[(s, a)] = t\n",
    "\n",
    "        # Update Q-values (Every-Visit MC)\n",
    "        G = 0\n",
    "        \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state_t, action_t, reward_t = episode[t]\n",
    "            G = reward_t + gamma * G\n",
    "            \n",
    "            # first visit or every-visit\n",
    "            if (FIRST_VISIT and t == first_occurrences.get((state_t, action_t), -1)) or not FIRST_VISIT:\n",
    "                returns_count[state_t][action_t] += 1\n",
    "                Q[state_t][action_t] += (G - Q[state_t][action_t]) / returns_count[state_t][action_t]  # Incremental average\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77021a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo for the GridWorld\n",
    "env = GridWorld(size=5)\n",
    "Q = monte_carlo_q_learning(env, episodes=10000)#,FIRST_VISIT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7dd5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state(0,0): right;  state(0,1): right;  state(0,2): right;  state(0,3): right;  state(0,4): down;  \n",
      "state(1,0): right;  state(1,1): right;  state(1,2): right;  state(1,3): right;  state(1,4): down;  \n",
      "state(2,0): down;  state(2,1): down;  state(2,2): up;  state(2,3): down;  state(2,4): down;  \n",
      "state(3,0): down;  state(3,1): down;  state(3,2): right;  state(3,3): right;  state(3,4): down;  \n",
      "state(4,0): right;  state(4,1): right;  state(4,2): right;  state(4,3): right;  state(4,4): up;  \n"
     ]
    }
   ],
   "source": [
    "# Check each state best action based on the greedy policy\n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state=(i,j)\n",
    "        action = np.argmax(Q[state])\n",
    "        print(f'state({i},{j}): {env.actions[action]}',end=';  ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c38032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
