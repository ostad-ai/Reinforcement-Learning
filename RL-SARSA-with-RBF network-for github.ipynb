{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6680cd49",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### SARSA With an RBF network\n",
    "\n",
    "We talked about **SARSA**, which is a **temporal difference** (TD) **model-free** method used in **Reinforcement Learning** (RL) to obtain the optimal policy for a **Markov Decision Process** (MDP). The SARSA uses the following iteration for updating action-value function $q(s,a)$:\n",
    "<br> $\\large q(s,a)\\leftarrow q(s,a)+\\alpha (r+\\gamma q(s',a')-q(s,a))$\n",
    "<br> where $s'$ is the next state, and $a'$ is the action chosen at state $s'$. Also, $r$ is the reward received after taking action $a$.\n",
    "<br> **Hint** Along with the algorithm SARSA, we use the ϵ-greedy for action-selection. We talked about ϵ-greedy in the previosu post.\n",
    "So far, we have used tables for $q$-values. But, this time we employ an RBF network to approximate $q(s,a)$ such that:\n",
    "<br> $\\large q(s,a)=F_a(\\boldsymbol{x}(s)))$\n",
    "<br>where $\\boldsymbol{x}(s)$ is the feature vector extracted from state $s$.\n",
    "We use SGD (stochastic gradient descent) to adjust weights of the RBF network.\n",
    "<hr>\n",
    "\n",
    "The example in this Notebook is almost the same **Grid World** we introduced earlier. geenrally, we can have a grid of any size\n",
    " - **States:** A sizexsize grid (size*size states), labeled as (0,0) to (size-1,size-1).\n",
    " - **Actions:** Up, Down, Left, Right.\n",
    " - **Rewards:**\n",
    "    - Reaching the goal state (size-1,size-1) gives a reward of +10.\n",
    "    - Reaching a \"pit\" state (size/2,size/2) gives a reward of −10.\n",
    "    - All other transitions give a reward of −1.\n",
    "- **Terminal States:** (size-1,size-1) (goal) and (size/2,size/2) (pit).\n",
    "- **Transition Probabilities:**\n",
    "    - Moving in the intended direction succeeds with probability 0.8.\n",
    "    - With probability 0.2, the agent moves in a random direction\n",
    "\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d39ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e533e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RBF network class with SGD and regularization\n",
    "# num_centers is also the number of neurons in the hidden layer\n",
    "class RBFNetwork:\n",
    "    def __init__(self, state_dim, action_dim, num_centers=20, sigma=1.):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_centers = num_centers\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Initialize RBF centers, weights, and biases\n",
    "        self.centers = np.random.rand(num_centers, state_dim)\n",
    "        self.weights = np.random.rand(num_centers, action_dim) * 0.001\n",
    "        self.biases = np.zeros(action_dim)  # Bias term for each action\n",
    "        \n",
    "    def rbf(self, x):\n",
    "        # Calculate RBF activations\n",
    "        x = np.array(x).reshape(1, -1)\n",
    "        distances = np.linalg.norm(self.centers - x, axis=1)\n",
    "        return np.exp(-self.sigma * distances**2)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Predict Q-values for all actions (including bias terms)\n",
    "        phi = self.rbf(x)\n",
    "        # Add biases to each output\n",
    "        return np.dot(phi, self.weights) + self.biases  \n",
    "    \n",
    "    def update(self, x, action, target, learning_rate):\n",
    "        # Update weights and biases using gradient descent\n",
    "        phi = self.rbf(x)\n",
    "        q_values = np.dot(phi, self.weights) + self.biases\n",
    "        error = target - q_values[action]\n",
    "        \n",
    "        # Update weights for the chosen action\n",
    "        self.weights[:, action] += learning_rate * error * phi\n",
    "        \n",
    "        # Update bias for the chosen action\n",
    "        self.biases[action] += learning_rate * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be97c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment as a grid world\n",
    "# the agent has four pissible actions\n",
    "# the grid contains size*size cells\n",
    "# We also define pit cell and goal cell\n",
    "# also we specify the rewards\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.action_map = {a:i for i,a in enumerate(self.actions)}\n",
    "        self.terminal = {(size-1, size-1): 10}  # Goal at bottom-right\n",
    "        self.pits = {(size//2, size//2): -10}   # Pit at center\n",
    "        self.current_state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = (0, 0)\n",
    "        return self._state_to_features(self.current_state)\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = self.actions[action_idx]\n",
    "        i, j = self.current_state\n",
    "\n",
    "        if self.current_state in self.terminal:\n",
    "            return self._state_to_features(self.current_state), 0, True\n",
    "\n",
    "        # Movement with stochasticity\n",
    "        if random.random() < 0.8:\n",
    "            next_state = self._move(action, i, j)\n",
    "        else:\n",
    "            next_state = self._move(random.choice(self.actions), i, j)\n",
    "\n",
    "        self.current_state = next_state\n",
    "        reward = self.terminal.get(next_state, self.pits.get(next_state, -1))\n",
    "        done = next_state in self.terminal or next_state in self.pits\n",
    "        return self._state_to_features(next_state), reward, done\n",
    "\n",
    "    def _move(self, action, i, j):\n",
    "        if action == 'up': return (max(i-1, 0), j)\n",
    "        elif action == 'down': return (min(i+1, self.size-1), j)\n",
    "        elif action == 'left': return (i, max(j-1, 0))\n",
    "        elif action == 'right': return (i, min(j+1, self.size-1))\n",
    "\n",
    "    def _state_to_features(self, state):\n",
    "        i, j = state\n",
    "        features = [\n",
    "            i / (self.size-1),\n",
    "            j / (self.size-1),\n",
    "            i / (self.size-1)*j / (self.size-1),\n",
    "            (self.size-1 - i) / (self.size-1),\n",
    "            (self.size-1 - j) / (self.size-1),\n",
    "            abs(i - self.size//2) / self.size,\n",
    "            abs(j - self.size//2) / self.size,\n",
    "            float(i in [0, self.size-1]),\n",
    "            float(j in [0, self.size-1])]\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78e28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to initiallzie RBF centers\n",
    "def initialize_rbf_centers(env, rbf_net, num_samples=1000):\n",
    "    \"\"\"Initialize RBF centers using random states from the environment\"\"\"\n",
    "    states = []\n",
    "    for _ in range(num_samples):\n",
    "        env.reset()\n",
    "        for _ in range(10):\n",
    "            action = random.randint(0, len(env.actions)-1)\n",
    "            state, _, done = env.step(action)\n",
    "            states.append(state)\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    states = np.unique(np.array(states), axis=0)  # Remove duplicate states\n",
    "    actual_centers = min(rbf_net.num_centers, len(states))  # Adjust centers based on unique states\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=actual_centers, n_init=10)  # Explicitly set n_init\n",
    "    kmeans.fit(states)\n",
    "    \n",
    "    # If we got fewer centers than requested, fill the rest with random states\n",
    "    if actual_centers < rbf_net.num_centers:\n",
    "        additional_centers = np.random.rand(rbf_net.num_centers - actual_centers, rbf_net.state_dim)\n",
    "        rbf_net.centers = np.vstack([kmeans.cluster_centers_, additional_centers])\n",
    "    else:\n",
    "        rbf_net.centers = kmeans.cluster_centers_\n",
    "    D = cdist(rbf_net.centers, rbf_net.centers)\n",
    "    np.fill_diagonal(D, np.inf)  # Ignore self-distance\n",
    "    rbf_net.sigma = np.mean(np.min(D, axis=1))  # Avg dist to nearest center  \n",
    "    \n",
    "# The epsilon-greedy action-selection policy\n",
    "def epsilon_greedy(rbf_net, state, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim-1)\n",
    "    else:\n",
    "        q_values = rbf_net.predict(state)\n",
    "        return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e13a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SARSA with the RBF network and SGD\n",
    "def train_sarsa_rbf(env, episodes=2000, batch_size=1, gamma=0.99, learning_rate=0.06):\n",
    "    state_dim = len(env._state_to_features((0,0)))\n",
    "    action_dim = len(env.actions)\n",
    "    agent = RBFNetwork(state_dim, action_dim, num_centers=25)  # Reduced centers to 25    \n",
    "    # Initialize RBF centers\n",
    "    initialize_rbf_centers(env, agent)    \n",
    "    buffer = deque(maxlen=10000)\n",
    "    epsilon = 1.0    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        action = epsilon_greedy(agent, state, epsilon, action_dim)\n",
    "        episode_reward = 0        \n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = epsilon_greedy(agent, next_state, epsilon, action_dim)            \n",
    "            # Store experience in buffer\n",
    "            buffer.append((state, action, reward, next_state, next_action, done))            \n",
    "            episode_reward += reward            \n",
    "            # Online update (batch_size=1)\n",
    "            if len(buffer) >= batch_size:\n",
    "                batch_samples = random.sample(buffer, batch_size)\n",
    "                for sample in batch_samples:\n",
    "                    s, a, r, s_next, a_next, d = sample                    \n",
    "                    # SARSA update\n",
    "                    current_q = agent.predict(s)[a]\n",
    "                    next_q = agent.predict(s_next)[a_next] if not d else 0\n",
    "                    target = r + gamma * next_q\n",
    "                    agent.update(s, a, target, learning_rate)            \n",
    "            if done: break                \n",
    "            state, action = next_state, next_action        \n",
    "        # Decay epsilon\n",
    "        epsilon=max(.01,1-episode/episodes)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.2f}\",end='; ')    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff788ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -60, Epsilon: 1.00; Episode 100, Reward: -28, Epsilon: 0.97; Episode 200, Reward: -48, Epsilon: 0.93; Episode 300, Reward: -13, Epsilon: 0.90; Episode 400, Reward: -46, Epsilon: 0.87; Episode 500, Reward: -24, Epsilon: 0.83; Episode 600, Reward: -31, Epsilon: 0.80; Episode 700, Reward: -15, Epsilon: 0.77; Episode 800, Reward: -25, Epsilon: 0.73; Episode 900, Reward: -19, Epsilon: 0.70; Episode 1000, Reward: -13, Epsilon: 0.67; Episode 1100, Reward: -9, Epsilon: 0.63; Episode 1200, Reward: -8, Epsilon: 0.60; Episode 1300, Reward: -13, Epsilon: 0.57; Episode 1400, Reward: -15, Epsilon: 0.53; Episode 1500, Reward: -15, Epsilon: 0.50; Episode 1600, Reward: 0, Epsilon: 0.47; Episode 1700, Reward: 2, Epsilon: 0.43; Episode 1800, Reward: 1, Epsilon: 0.40; Episode 1900, Reward: -15, Epsilon: 0.37; Episode 2000, Reward: -17, Epsilon: 0.33; Episode 2100, Reward: 0, Epsilon: 0.30; Episode 2200, Reward: 1, Epsilon: 0.27; Episode 2300, Reward: -11, Epsilon: 0.23; Episode 2400, Reward: -13, Epsilon: 0.20; Episode 2500, Reward: 3, Epsilon: 0.17; Episode 2600, Reward: 2, Epsilon: 0.13; Episode 2700, Reward: -13, Epsilon: 0.10; Episode 2800, Reward: -4, Epsilon: 0.07; Episode 2900, Reward: -7, Epsilon: 0.03; \n",
      "---------Testing trained policy:\n",
      "Step 1: At (0, 1), took right, Reward: -1\n",
      "Step 2: At (0, 2), took right, Reward: -1\n",
      "Step 3: At (0, 3), took right, Reward: -1\n",
      "Step 4: At (0, 4), took right, Reward: -1\n",
      "Step 5: At (1, 4), took down, Reward: -1\n",
      "Step 6: At (2, 4), took down, Reward: -1\n",
      "Step 7: At (3, 4), took down, Reward: -1\n",
      "Step 8: At (3, 4), took down, Reward: -1\n",
      "Step 9: At (4, 4), took down, Reward: 10\n",
      "\n",
      "Total reward: 2\n",
      "Reached goal in 9 steps\n"
     ]
    }
   ],
   "source": [
    "# The function to test the learned policy\n",
    "def test_policy(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    print(\"\\n---------Testing trained policy:\")\n",
    "    while not done:\n",
    "        action = np.argmax(agent.predict(state))\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        print(f\"Step {steps}: At {env.current_state}, took {env.actions[action]}, Reward: {reward}\")\n",
    "    \n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "    print(f\"Reached goal in {steps} steps\")\n",
    "\n",
    "# The main function to call\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld(size=5)\n",
    "    trained_agent = train_sarsa_rbf(env, episodes=3000)\n",
    "    test_policy(env, trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004b9dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state(0,0): right,state(0,1): right,state(0,2): right,state(0,3): right,state(0,4): down,\n",
      "state(1,0): down,state(1,1): right,state(1,2): right,state(1,3): right,state(1,4): down,\n",
      "state(2,0): down,state(2,1): right,state(2,2): right,state(2,3): right,state(2,4): down,\n",
      "state(3,0): right,state(3,1): right,state(3,2): right,state(3,3): right,state(3,4): down,\n",
      "state(4,0): right,state(4,1): right,state(4,2): right,state(4,3): right,state(4,4): left,\n"
     ]
    }
   ],
   "source": [
    "# Check each state best action based on the greedy policy\n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state=env._state_to_features((i,j))\n",
    "        action = np.argmax(trained_agent.predict(state))\n",
    "        print(f'state({i},{j}): {env.actions[action]}',end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257d742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
