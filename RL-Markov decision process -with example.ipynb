{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "753208ce",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Markov Decision Process (MDP)\n",
    "A **Markov decision process** (MDP) is a mathematical framework used for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker.\n",
    "<br>MDPs are used in a wide range of applications, including:\n",
    "- Robotics (path planning, control),\n",
    "- Game AI (decision-making in games),\n",
    "- Finance (portfolio management),\n",
    "- Healthcare (treatment planning),\n",
    "- Autonomous systems (self-driving cars).\n",
    "\n",
    "The **Markov property** is a key assumption in MDPs that ensures the future depends only on the current state and action, not on the history of past states and actions. This property simplifies the modeling and solving of sequential decision-making problems and is the foundation of many algorithms in reinforcement learning and dynamic programming.\n",
    "<hr> \n",
    "\n",
    "In a Markov Decision Process (MDP), the **agent-enviornment interaction** follows a sequential decision-making process. The agent (e.g., the robot) interacts with the environment (the grid world, for example) by taking actions, transitioning between states, and receiving rewards.\n",
    "<br> An example is given for a 2-by-2 grid world in which a robot can take one of four actions *Up*, *Down*, *Left*, and *Right*. Based on the curent state and chosen action, the state changes and a reward is received. The state-transition is goverened by transition probabilties defined for our grid world (the transition probabilitise are fictional). Morover, the *goal state* is state 4 of the grid world. \n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b6dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required module\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d17e6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# Simulates the next state and reward based on the current state and action\n",
    "# for the grid world 2*2.\n",
    "# The transition probabilities are fictional\n",
    "def grid_world_transition(state, action):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (int): Current state (1, 2, 3, or 4).\n",
    "        actions (str): (\"Up\", \"Down\", \"Left\", or \"Right\").\n",
    "    \n",
    "    Returns:\n",
    "        next_state (int): Next state.\n",
    "        reward (int): Reward received.\n",
    "    \"\"\"\n",
    "    if state == 1:\n",
    "        if action == \"Right\":\n",
    "            next_state = random.choices([2, 1, 3], weights=[0.7, 0.2, 0.1])[0]\n",
    "            reward = -1\n",
    "        elif action == \"Down\":\n",
    "            next_state = 3\n",
    "            reward = -1\n",
    "        else:\n",
    "            next_state = 1\n",
    "            reward = -1\n",
    "    \n",
    "    elif state == 2:\n",
    "        if action == \"Down\":\n",
    "            next_state = random.choices([4, 2], weights=[0.8, 0.2])[0]\n",
    "            reward = 10 if next_state == 4 else -1\n",
    "        elif action == \"Left\":\n",
    "            next_state = 1\n",
    "            reward = -1\n",
    "        else:\n",
    "            next_state = 2\n",
    "            reward = -1\n",
    "    \n",
    "    elif state == 3:\n",
    "        if action == \"Up\":\n",
    "            next_state = random.choices([1, 3], weights=[0.8, 0.2])[0]\n",
    "            reward = -1\n",
    "        elif action == \"Right\":\n",
    "            next_state = 4\n",
    "            reward = 10\n",
    "        else:\n",
    "            next_state = 3\n",
    "            reward = -1\n",
    "    \n",
    "    elif state == 4:\n",
    "        next_state = 4\n",
    "        reward = 10\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid state. State must be 1, 2, 3, or 4.\")\n",
    "    \n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea643d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: 1, Action: Right\n",
      "Next State: 2, Reward: -1\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "current_state = random.choice([1,2,3,4])\n",
    "action = random.choice([\"Left\",\"Right\",\"Up\", \"Down\"])\n",
    "next_state, reward = grid_world_transition(current_state, action)\n",
    "print(f\"Current State: {current_state}, Action: {action}\")\n",
    "print(f\"Next State: {next_state}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080ec90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
