{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f715227f",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### The REINFORCE method: A policy gradient method\n",
    "\n",
    "The **REINFORCE** is a **policy gradient** algorithm that:  \n",
    "1. **Directly optimizes** a stochastic policy by estimating gradients from **complete episodes**.  \n",
    "2. Uses **Monte Carlo returns** $G_t$ to weight action probabilities, favoring high-reward trajectories.  \n",
    "3. Updates policy parameters via gradient *ascent* on $\\nabla_{\\theta} \\, log (\\pi_{\\theta}(a_t|s_t)) \\cdot G_t$.  \n",
    "4. Requires **no value function** (vanilla version).  \n",
    "\n",
    "Here, we use a MLP for approximationg policies, which we call it the policy network. The policy network is trained by the gradient ascent for the GridWorld we have used so far. \n",
    "<hr>\n",
    "\n",
    "We use the same **Grid World** environment (similar to the one we used before) but wiht increased rewards to help REINFORCE:\n",
    " - **States:** A sizexsize grid (size*size states), labeled as (0,0) to (size-1,size-1).\n",
    " - **Actions:** Up, Down, Left, Right.\n",
    " - **Rewards:**\n",
    "    - Reaching the goal state (size-1,size-1) gives a reward of +1000.\n",
    "    - Reaching a \"pit\" state (size//2,size//2) gives a reward of −1000.\n",
    "    - All other transitions give a reward of −1.\n",
    "- **Terminal States:** (size-1,size-1) (goal) and (size//2,size//2) (pit).\n",
    "- **Transition Probabilities:**\n",
    "    - Moving in the intended direction succeeds with probability 0.8.\n",
    "    - With probability 0.2, the agent moves in a random direction\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the following, we first implement the GridWrold. Then, we train the policy network by the REINFORCE.\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ef6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcc597d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld environment with increased rewards\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.terminal = {(size-1, size-1): 1000.}  # Goal at bottom-right\n",
    "        self.pits = {(size//2, size//2): -1000.}   # Pit at center\n",
    "        self.current_state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = (0, 0)\n",
    "        return self._state_to_features(self.current_state)\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = self.actions[action_idx]\n",
    "        i, j = self.current_state\n",
    "\n",
    "        if self.current_state in self.terminal:\n",
    "            return self._state_to_features(self.current_state), 0, True\n",
    "\n",
    "        # Movement with stochasticity\n",
    "        if random.random() < 0.8:\n",
    "            next_state = self._move(action, i, j)\n",
    "        else:\n",
    "            next_state = self._move(random.choice(self.actions), i, j)\n",
    "\n",
    "        self.current_state = next_state\n",
    "        reward = self.terminal.get(next_state, self.pits.get(next_state, -1.))\n",
    "        done = next_state in self.terminal or next_state in self.pits\n",
    "        return self._state_to_features(next_state), reward, done\n",
    "\n",
    "    def _move(self, action, i, j):\n",
    "        if action == 'up': return (max(i-1, 0), j)\n",
    "        elif action == 'down': return (min(i+1, self.size-1), j)\n",
    "        elif action == 'left': return (i, max(j-1, 0))\n",
    "        elif action == 'right': return (i, min(j+1, self.size-1))\n",
    "\n",
    "    def _state_to_features(self, state):\n",
    "        \"\"\"Enhanced features for larger grids\"\"\"\n",
    "        i, j = state\n",
    "        features = [\n",
    "            i / (self.size-1),  # Normalized x\n",
    "            j / (self.size-1),  # Normalized y\n",
    "             i / (self.size-1)*j / (self.size-1),  \n",
    "            (self.size-1 - i) / (self.size-1),  # Progress right\n",
    "            (self.size-1 - j) / (self.size-1),  # Progress down\n",
    "            abs(i - self.size//2) / self.size,  # Pit x-distance\n",
    "            abs(j - self.size//2) / self.size,  # Pit y-distance\n",
    "            float(i in [0, self.size-1]),  # Edge x\n",
    "            float(j in [0, self.size-1])   # Edge y\n",
    "        ]\n",
    "        return torch.FloatTensor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c7fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy network (last layer uses softmax)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size,hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.net(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbc4d950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Avg Reward: -607.5\n",
      "Episode 400, Avg Reward: -344.6\n",
      "Episode 600, Avg Reward: -382.1\n",
      "Episode 800, Avg Reward: -281.7\n",
      "Episode 1000, Avg Reward: -441.1\n",
      "Episode 1200, Avg Reward: 37.6\n",
      "Episode 1400, Avg Reward: -78.7\n",
      "Episode 1600, Avg Reward: 78.2\n",
      "Episode 1800, Avg Reward: 81.1\n",
      "Episode 2000, Avg Reward: 184.4\n",
      "Episode 2200, Avg Reward: 86.0\n",
      "Episode 2400, Avg Reward: 223.2\n",
      "Episode 2600, Avg Reward: 165.9\n",
      "Episode 2800, Avg Reward: 105.7\n",
      "Episode 3000, Avg Reward: 286.7\n",
      "Episode 3200, Avg Reward: 148.3\n",
      "Episode 3400, Avg Reward: 246.4\n",
      "Episode 3600, Avg Reward: 328.5\n",
      "Episode 3800, Avg Reward: 307.8\n",
      "Episode 4000, Avg Reward: 267.6\n",
      "Episode 4200, Avg Reward: 368.7\n",
      "Episode 4400, Avg Reward: 428.7\n",
      "Episode 4600, Avg Reward: 368.6\n",
      "Episode 4800, Avg Reward: 449.5\n",
      "Episode 5000, Avg Reward: 249.6\n"
     ]
    }
   ],
   "source": [
    "# The main training function\n",
    "def train_reinforce(env, policy, optimizer, episodes=1000, gamma=0.99):\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "        # Generate an episode\n",
    "        while True:\n",
    "            probs = policy(state)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_probs.append(m.log_prob(action))\n",
    "            next_state, reward, done = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            state = next_state.clone()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        \n",
    "        # Calculate loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track rewards\n",
    "        total_reward = sum(rewards)\n",
    "        rewards_history.append(total_reward)\n",
    "        \n",
    "        if (episode+1) % 200 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-100:])\n",
    "            print(f\"Episode {episode+1}, Avg Reward: {avg_reward:.1f}\")\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Initialize\n",
    "env = GridWorld(size=5)\n",
    "policy = PolicyNetwork(input_size=9, output_size=4)  # feature vector as input\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.00002)\n",
    "\n",
    "# Train\n",
    "history = train_reinforce(env, policy, optimizer, episodes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f59d964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state(0,0): right,state(0,1): right,state(0,2): right,state(0,3): right,state(0,4): down,\n",
      "state(1,0): down,state(1,1): right,state(1,2): right,state(1,3): down,state(1,4): down,\n",
      "state(2,0): down,state(2,1): right,state(2,2): right,state(2,3): right,state(2,4): down,\n",
      "state(3,0): down,state(3,1): right,state(3,2): right,state(3,3): right,state(3,4): down,\n",
      "state(4,0): right,state(4,1): right,state(4,2): right,state(4,3): right,state(4,4): right,\n"
     ]
    }
   ],
   "source": [
    "# The greedy policy learnt for each state of the environment. \n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state=[i,j]\n",
    "        state_tensor=env._state_to_features(state)\n",
    "        #state_feature=env.get_state()\n",
    "        #state_tensor = torch.FloatTensor(state_feature)\n",
    "        with torch.no_grad():\n",
    "            action_probs = policy(state_tensor)\n",
    "        action = torch.argmax(action_probs).item()\n",
    "        action_str = env.actions[action]\n",
    "        print(f'state({i},{j}): {action_str}',end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbf7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
