{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a9125a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Value iteration\n",
    "\n",
    "The **Value Iteration** is a *dynamic programming* algorithm (a model-based method) used in **Reinforcement Learning** (RL) to compute the optimal policy for a **Markov Decision Process** (MDP). The algorithm iteratively improves the value function until it converges to the optimal value function, from which the optimal policy can be derived.\n",
    "<br>The key steps in **Value Iteration**:\n",
    " - **Initialize:** Start with an arbitrary value function $v(s)$ for all states $s$.\n",
    " - **Iterate:** Update the value function using the **Bellman optimality equation**:\n",
    "    <br> $v_{k+1}(s)=max_⁡a \\sum_{s′} p(s′|s,a)[r(s,a,s′)+\\gamma v_k(s′)]$\n",
    "    <br> where:\n",
    "    <br> $p(s'|s,a)$ is the transition probability.\n",
    "    <br> $r(s,a,s′)$ is the reward function.\n",
    "    <br> $\\gamma$ is the discount factor.\n",
    " - **Convergence:** Repeat the iteration until the value function converges (i.e., the change between iterations is smaller than a threshold $\\theta$).\n",
    " - **Extract Policy:** Once the value function converges, extract the optimal policy $\\pi(s)$ by selecting the action that maximizes the expected value.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the example in this Notebook, we use a **Grid World** environment:\n",
    " - **States:** A 3x3 grid (9 states), labeled as (0,0) to (2,2).\n",
    " - **Actions:** Up, Down, Left, Right.\n",
    " - **Rewards:**\n",
    "    - Reaching the goal state (2,2) gives a reward of +10.\n",
    "    - Reaching a \"pit\" state (1,1) gives a reward of −10.\n",
    "    - All other transitions give a reward of −1.\n",
    "- **Terminal States:** (2,2) (goal) and (1,1) (pit).\n",
    "- **Transition Probabilities:**\n",
    "    - Moving in the intended direction succeeds with probability 0.8.\n",
    "    - With probability 0.2, the agent moves in a random direction\n",
    "\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645d074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid world\n",
    "states = [(i, j) for i in range(3) for j in range(3)]  # 3x3 grid\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]              # Possible actions\n",
    "gamma = 0.9                                           # Discount factor\n",
    "theta = 1e-6                                          # Convergence threshold\n",
    "\n",
    "# Terminal states\n",
    "terminal_states = {(2, 2): 10, (1, 1): -10}  # (state: reward)\n",
    "\n",
    "# Transition probabilities\n",
    "def transition_probability(s, a, s_):\n",
    "    if s in terminal_states:  # Terminal states have no transitions\n",
    "        return 0\n",
    "    intended_s = get_intended_state(s, a)\n",
    "    if s_ == intended_s:\n",
    "        return 0.8\n",
    "    elif s_ in get_neighbors(s):\n",
    "        return 0.2 / len(get_neighbors(s))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Reward function\n",
    "def reward(s, a, s_):\n",
    "    if s_ in terminal_states:\n",
    "        return terminal_states[s_]\n",
    "    return -1  # Default reward for non-terminal transitions\n",
    "\n",
    "# The state the agent goes to, \n",
    "# from current state s, by taking action a\n",
    "# if everything was deterministic\n",
    "def get_intended_state(s, a):\n",
    "    i, j = s\n",
    "    if a == \"up\":\n",
    "        return (max(i - 1, 0), j)\n",
    "    elif a == \"down\":\n",
    "        return (min(i + 1, 2), j)\n",
    "    elif a == \"left\":\n",
    "        return (i, max(j - 1, 0))\n",
    "    elif a == \"right\":\n",
    "        return (i, min(j + 1, 2))\n",
    "\n",
    "# Immediate neighbors of the current state s \n",
    "def get_neighbors(s):\n",
    "    i, j = s\n",
    "    neighbors = []\n",
    "    for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < 3 and 0 <= nj < 3:\n",
    "            neighbors.append((ni, nj))\n",
    "    return neighbors\n",
    "\n",
    "# Value function initialization\n",
    "v = {s: 0 for s in states}\n",
    "\n",
    "# Value Iteration\n",
    "def value_iteration():\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if s in terminal_states:  # Skip terminal states\n",
    "                continue\n",
    "            v_current = v[s]\n",
    "            v[s] = max(sum(transition_probability(s, a, s_) *\\\n",
    "                  (reward(s, a, s_) + gamma * v[s_])\\\n",
    "                for s_ in states)\\\n",
    "                for a in actions)\n",
    "            delta = max(delta, abs(v_current - v[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "# Extract optimal policy\n",
    "def extract_policy():\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        if s in terminal_states:  # No action for terminal states\n",
    "            policy[s] = None\n",
    "            continue\n",
    "        policy[s] = max(actions,\n",
    "            key=lambda a: sum(transition_probability(s, a, s_) *\\\n",
    "            (reward(s, a, s_) + gamma * v[s_])\\\n",
    "            for s_ in states))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf0a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: A 3*3 Grid World\n",
      "Algorithm: Value Iteration\n",
      "------------------------------\n",
      "Optimal Value Function v(s):\n",
      "State (0, 0): 0.63, State (0, 1): 1.89, State (0, 2): 4.71, \n",
      "State (1, 0): 1.89, State (1, 1): 0.00, State (1, 2): 7.55, \n",
      "State (2, 0): 4.71, State (2, 1): 7.55, State (2, 2): 0.00, \n",
      "\n",
      "Optimal Policy π(s):\n",
      "State (0, 0): down, State (0, 1): right, State (0, 2): down, \n",
      "State (1, 0): down, State (1, 1): None, State (1, 2): down, \n",
      "State (2, 0): right, State (2, 1): right, State (2, 2): None, \n"
     ]
    }
   ],
   "source": [
    "# Run value iteration\n",
    "value_iteration()\n",
    "# Extract and print the optimal policy\n",
    "optimal_policy = extract_policy()\n",
    "print('Environment: A 3*3 Grid World')\n",
    "print('Algorithm: Value Iteration')\n",
    "print(30*'-')\n",
    "print(\"Optimal Value Function v(s):\")\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        print(f\"State {(row,col)}: {v[(row,col)]:.2f}\",end=', ')\n",
    "    print('')\n",
    "print(\"\\nOptimal Policy \\u03c0(s):\")\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        print(f\"State {(row,col)}: {optimal_policy[(row,col)]}\",end=', ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96644456-a975-498c-9bc2-1df4d21fba77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
