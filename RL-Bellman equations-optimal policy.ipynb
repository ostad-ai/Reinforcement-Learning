{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f474bed",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Bellman equations, Bellman optimality equations, and optimal policy\n",
    "With **Bellman equations**, we get the **Bellman optimality equations**. With the Bellman optimality equations, we can estimate the **optimal value functions**. As a result, the **optimal policy** can be obtained from the optimal value functions. This is one way to solve an RL problem.\n",
    "<hr>\n",
    "\n",
    "The Bellman equation for **state-value function** $v_\\pi(s)$:\n",
    "<br>$v_\\pi(s)=\\sum_a \\pi(a|s)\\sum_{s′,r} p(s′,r|s,a)(r+\\gamma v_\\pi(s′))$\n",
    "<br>Which states that the value of a state $s$ is the expected immediate reward plus the discounted value of the next state $s′$, averaged over all possible actions and next states.\n",
    "<br><br> The Bellman equation for **action-value function** $q_\\pi(s,a)$\n",
    "<br>$q_\\pi(s,a)=\\sum_{s′,r} p(s′,r|s,a)(r+\\gamma \\sum_{a'} \\pi(a'|s') q_\\pi(s′,a'))$\n",
    "<br>Which states that the value of taking action aa in state $s$ is the expected immediate reward plus the discounted value of the next state $s′$, averaged over all possible next states and actions.\n",
    "<hr>\n",
    "\n",
    "Specifically, the optimal value functions $v_*$ or $q_*$ satisfy the Bellman optimality equations, which are recursive relationships that define the optimal values in terms of each other.\n",
    "<br>a. **Bellman Optimality Equation** for $v_*(s)$:\n",
    "<br>$v_*(s)=max⁡_a \\sum_{s′,r} p(s′,r|s,a)(r+\\gamma v_*(s′))$\n",
    "<br>b. **Bellman Optimality Equation** for $q_*(s,a)$:\n",
    "<br>$q_*(s,a)=\\sum_{s',r} p(s′,r|s,a)(r+\\gamma max_{a'} q_*(s′,a′))$\n",
    "<hr>\n",
    "\n",
    "An **optimal policy** $\\pi_*$ is defined as a policy that maximizes the expected return (cumulative discounted reward) for all states $s$. Having the optimal value functions, we can get the optimal policy $\\pi_*$ by:\n",
    "<br>$\\pi_*(s)=argmax⁡_a \\sum_{s′,r} p(s′,r|s,a)(r+\\gamma v_*(s′))$\n",
    "<br> Or from the action-value function:\n",
    "<br>$\\pi_*(s)=argmax_a q_*(s,a)$\n",
    "<hr>\n",
    "In the following, we use the Bellman optimality equation to get the optimal state-value function $v_*(s)$. Next, we obtain the optimal policy by $v_*$. This example is also a little advanced at this stage. But, it gives some hints about the formulae mentioned here. \n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08fa190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an MDP with two states and two actions\n",
    "# We use the three-argument transition probabilities\n",
    "# Define the MDP parameters\n",
    "states = ['s1', 's2']\n",
    "actions = ['a1', 'a2']\n",
    "\n",
    "# Transition probabilities P(s' | s, a)\n",
    "transition_probs = {\n",
    "    's1': {'a1': {'s2': 1.0}, 'a2': {'s1': 1.0}},\n",
    "    's2': {'a1': {'s1': 1.0}, 'a2': {'s2': 1.0}},\n",
    "}\n",
    "\n",
    "# Rewards R(s, a, s')\n",
    "rewards = {\n",
    "    's1': {'a1': {'s2': 5}, 'a2': {'s1': 1}},\n",
    "    's2': {'a1': {'s1': 3}, 'a2': {'s2': 2}},\n",
    "}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Value iteration to compute V*(s)\n",
    "def value_iteration(states, actions, transition_probs, rewards, gamma, theta=1e-6):\n",
    "    V = {s: 0 for s in states}  # Initialize V(s) to 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V[s]\n",
    "            # Compute the new value for V[s] using the Bellman optimality equation\n",
    "            V[s] = max(\n",
    "                sum(\n",
    "                    transition_probs[s][a].get(s_next, 0) *\\\n",
    "                    (rewards[s][a].get(s_next, 0) + gamma * V[s_next])\n",
    "                    for s_next in states\n",
    "                )\n",
    "                for a in actions\n",
    "            )\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Derive the optimal policy pi*(s) from V*(s)\n",
    "def get_optimal_policy(states, actions, transition_probs, rewards, gamma, V):\n",
    "    policy = {}\n",
    "    for s in states:\n",
    "        # Choose the action that maximizes the expected return\n",
    "        policy[s] = max(\n",
    "            actions,\n",
    "            key=lambda a: sum(\n",
    "                transition_probs[s][a].get(s_next, 0) *\\\n",
    "                (rewards[s][a].get(s_next, 0) + gamma * V[s_next])\n",
    "                for s_next in states\n",
    "            )\n",
    "        )\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82091695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "V*(s1) = 40.53\n",
      "V*(s2) = 39.47\n",
      "\n",
      "Optimal Policy:\n",
      "pi*(s1) = a1\n",
      "pi*(s2) = a1\n"
     ]
    }
   ],
   "source": [
    "# Compute the optimal value function V*(s)\n",
    "V_optimal = value_iteration(states, actions, transition_probs, rewards, gamma)\n",
    "\n",
    "# Get the optimal policy using the optimal state-value function V_optimal\n",
    "optimal_policy = get_optimal_policy(states, actions, transition_probs, rewards, gamma, V_optimal)\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimal Value Function:\")\n",
    "for s in states:\n",
    "    print(f\"V*({s}) = {V_optimal[s]:.2f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for s in states:\n",
    "    print(f\"pi*({s}) = {optimal_policy[s]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc039db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
