{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93acaa0d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "## Returns, policy, and value functions\n",
    "\n",
    "The **return** $G_t$ is the total discounted reward the agent accumulates over time, starting from time step $t$. It is defined as:\n",
    "<br> $G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+... $\n",
    "<br>where $\\gamma$ is the discount rate such that $0\\le \\gamma \\le 1$.\n",
    "<br> There is a recursive relation between returns as specified below:\n",
    "<br>$G_t=R_{t+1}+\\gamma G_{t+1}$\n",
    "---\n",
    "A **policy** is a strategy that the agent uses to decide which actions to take in each state. It can be *deterministic* or *stochastic*.\n",
    "- **Determinitstic policy** maps each state to a specific action: $\\pi(s)=a$\n",
    "- **Stochastic policy** maps each stte to a probability distirbution over actions: $\\pi(a|s)=probability(A_t=a|S_t=s)$\n",
    "---\n",
    "**Reward hypothesis:** The reward hypothesis assumes that any goal or objective an agent might have can be expressed as the maximization of the expected return $E[G_t]$.\n",
    " - This hypothesis simplifies the problem of defining goals to the problem of designing a reward function that aligns with the desired behavior.\n",
    "\n",
    "---\n",
    "**Value functions** are used to evaluate how good a state or action is under a given policy. There are two main types of value functions:\n",
    "<br>The **state-value** function $v_\\pi (s)$ represents the expected return (cumulative discounted reward) when starting in state $s$ and following policy π thereafter:\n",
    "<br>$v_\\pi (s)=E_\\pi[G_t|S_t=s]$\n",
    "<br>The **action-value** function $q_\\pi(s,a)$ represents the expected return when starting in state $s$, taking action $a$, and following policy π thereafter:\n",
    "<br>$q_\\pi (s,a)=E_\\pi[G_t|S_t=s,A_t=a]$\n",
    "\n",
    "---\n",
    "In the following, a Grid world of size 3-by-3 is defined in which a robot can select one of four pssible actions in each cell of the grid. The rewards and the discount rate $\\gamma$ are also defined. A random policy is employed for the robot. The robot with the random policy explores the grid world and collects the returns for each cell gradually. THe returns are averaged to estimate the value functions of the cells. Finally, with the help of value functions, the best policy is found.\n",
    "<br> **Hint:** This example is a bit advanced at this stage, but it gives a good knowledge of the topcis we have covered so far.\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27dab260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59ef90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GridWorld environment\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = (3, 3)\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (2, 2)\n",
    "        self.current_state = self.start_state\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.rewards = {\n",
    "            self.goal_state: 10,  # Reward for reaching the goal\n",
    "            \"default\": -1         # Reward for all other steps\n",
    "        }\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the start state.\"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment based on the action.\"\"\"\n",
    "        x, y = self.current_state\n",
    "\n",
    "        # Perform the action\n",
    "        if action == \"up\":\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == \"down\":\n",
    "            x = min(x + 1, self.grid_size[0] - 1)\n",
    "        elif action == \"left\":\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == \"right\":\n",
    "            y = min(y + 1, self.grid_size[1] - 1)\n",
    "\n",
    "        self.current_state = (x, y)\n",
    "\n",
    "        # Check if the goal is reached\n",
    "        if self.current_state == self.goal_state:\n",
    "            reward = self.rewards[self.goal_state]\n",
    "            done = True\n",
    "        else:\n",
    "            reward = self.rewards[\"default\"]\n",
    "            done = False\n",
    "\n",
    "        return self.current_state, reward, done\n",
    "\n",
    "# Define a random policy\n",
    "def random_policy():\n",
    "    return np.random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "\n",
    "# Simulate episodes and compute returns, value functions, and policy\n",
    "def simulate_episodes(env, num_episodes=500):\n",
    "    returns = {}  # Stores cumulative returns for each state\n",
    "    value_function = np.zeros(env.grid_size)  # Stores the value function\n",
    "    state_counts = np.zeros(env.grid_size)   # Counts visits to each state\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Generate an episode\n",
    "        while not done:\n",
    "            action = random_policy()\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        # Compute returns for each state in the episode\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = reward + env.gamma * G\n",
    "\n",
    "            # Update returns and value function\n",
    "            if state not in returns:\n",
    "                returns[state] = []\n",
    "            returns[state].append(G)\n",
    "            value_function[state] = np.mean(returns[state])\n",
    "            state_counts[state] += 1\n",
    "\n",
    "    # Estimate the policy (greedy policy based on value function)\n",
    "    policy = np.empty(env.grid_size, dtype=object)\n",
    "    for x in range(env.grid_size[0]):\n",
    "        for y in range(env.grid_size[1]):\n",
    "            if (x, y) == env.goal_state:\n",
    "                policy[x, y] = \"goal\"\n",
    "                continue\n",
    "\n",
    "            # Find the action that maximizes the value of the next state\n",
    "            best_action = None\n",
    "            best_value = -np.inf\n",
    "            for action in env.actions:\n",
    "                next_x, next_y = x, y\n",
    "                if action == \"up\":\n",
    "                    next_x = max(x - 1, 0)\n",
    "                elif action == \"down\":\n",
    "                    next_x = min(x + 1, env.grid_size[0] - 1)\n",
    "                elif action == \"left\":\n",
    "                    next_y = max(y - 1, 0)\n",
    "                elif action == \"right\":\n",
    "                    next_y = min(y + 1, env.grid_size[1] - 1)\n",
    "\n",
    "                # Get the reward for taking this action\n",
    "                if (next_x, next_y) == env.goal_state:\n",
    "                    reward = env.rewards[env.goal_state]\n",
    "                else:\n",
    "                    reward = env.rewards[\"default\"]\n",
    "\n",
    "                # Get the value of the next state\n",
    "                next_state_value = value_function[next_x, next_y]\n",
    "\n",
    "                # Compute the total value: reward + discounted value of the next state\n",
    "                total_value = reward + env.gamma * next_state_value\n",
    "\n",
    "                # Update the best action if this action leads to a higher total value\n",
    "                if total_value > best_value:\n",
    "                    best_value = total_value\n",
    "                    best_action = action\n",
    "\n",
    "            policy[x, y] = best_action\n",
    "\n",
    "    return value_function, policy, state_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e3be74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "[[-5.9999017  -5.12966767 -3.86475725]\n",
      " [-5.02454069 -3.12148514  0.32475494]\n",
      " [-3.6617277   0.35562481  0.        ]]\n",
      "\n",
      "Policy:\n",
      "[['down' 'down' 'down']\n",
      " ['right' 'down' 'down']\n",
      " ['right' 'right' 'goal']]\n",
      "\n",
      "State Counts (Visits):\n",
      "[[6135. 3993. 3181.]\n",
      " [4011. 2946. 1950.]\n",
      " [2953. 1928.    0.]]\n"
     ]
    }
   ],
   "source": [
    "# Run the simulation\n",
    "env = GridWorld()\n",
    "value_function, policy, state_counts = simulate_episodes(env, num_episodes=1000)\n",
    "\n",
    "# Print results\n",
    "print(\"Value Function:\")\n",
    "print(value_function)\n",
    "print(\"\\nPolicy:\")\n",
    "print(policy)\n",
    "print(\"\\nState Counts (Visits):\")\n",
    "print(state_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b244f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
