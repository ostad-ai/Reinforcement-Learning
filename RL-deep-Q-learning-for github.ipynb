{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a9125a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Deep Q-learning : The Q-learning with deep learning (an MLP for regression)\n",
    "\n",
    "**Deep Q-Learning** is a *reinforcement learning* algorithm that combines **Q-learning** with **deep neural networks** to handle environments with high-dimensional state spaces (e.g., images). Instead of using a **Q-table**, a deep neural network approximates the Q-value function, predicting the value of each action for a given state. The agent learns by minimizing the difference between predicted and target Q-values using **experience replay** and a separate **target network** to stabilize training. \n",
    "<br> In fact, the ϵ-greedy action selection uses a **policy network** (an MLP for regression) and this policy network is updated by samples from the environment. However, from time to time, the parameters of the policy network is coped to the target network. This target network is used only to form the q-target for each transition (s,a,r,s',done). The **done** is true when s' is a terminal state. Otherwise, it is false. \n",
    "<br>Hint:THe policy network network is to approximate $q(s,a)$ such that:\n",
    "<br> $\\large q(s,a)=F_a(\\boldsymbol{x}(s)))$\n",
    "<br>where $\\boldsymbol{x}(s)$ is the feature vector extracted from state $s$. And $F_a(\\boldsymbol{x}(s)))$ is the $a$th component of the output vector $F(\\boldsymbol{x}(s)))$ of the policy network. \n",
    "<hr>\n",
    "\n",
    "The example in this Notebook is the same **Grid World** we used for the SARSA with RBF network. generally, we can have a grid of any size\n",
    " - **States:** A sizexsize grid (size*size states), labeled as (0,0) to (size-1,size-1).\n",
    " - **Actions:** Up, Down, Left, Right.\n",
    " - **Rewards:**\n",
    "    - Reaching the goal state (size-1,size-1) gives a reward of +10.\n",
    "    - Reaching a \"pit\" state (size/2,size/2) gives a reward of −10.\n",
    "    - All other transitions give a reward of −1.\n",
    "- **Terminal States:** (size-1,size-1) (goal) and (size/2,size/2) (pit).\n",
    "- **Transition Probabilities:**\n",
    "    - Moving in the intended direction succeeds with probability 0.8.\n",
    "    - With probability 0.2, the agent moves in a random direction\n",
    "\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba56c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb1d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GridWorld environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.terminal = {(size-1, size-1): 10}  # Goal at bottom-right\n",
    "        self.pits = {(size//2, size//2): -10}   # Pit at center\n",
    "        self.current_state = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = (0, 0)\n",
    "        return self._state_to_features(self.current_state)\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        action = self.actions[action_idx]\n",
    "        i, j = self.current_state\n",
    "\n",
    "        if self.current_state in self.terminal:\n",
    "            return self._state_to_features(self.current_state), 0, True\n",
    "\n",
    "        # Movement with stochasticity\n",
    "        if random.random() < 0.8:\n",
    "            next_state = self._move(action, i, j)\n",
    "        else:\n",
    "            next_state = self._move(random.choice(self.actions), i, j)\n",
    "\n",
    "        self.current_state = next_state\n",
    "        reward = self.terminal.get(next_state, self.pits.get(next_state, -1))\n",
    "        done = next_state in self.terminal or next_state in self.pits\n",
    "        return self._state_to_features(next_state), reward, done\n",
    "\n",
    "    def _move(self, action, i, j):\n",
    "        if action == 'up': return (max(i-1, 0), j)\n",
    "        elif action == 'down': return (min(i+1, self.size-1), j)\n",
    "        elif action == 'left': return (i, max(j-1, 0))\n",
    "        elif action == 'right': return (i, min(j+1, self.size-1))\n",
    "\n",
    "    def _state_to_features(self, state):\n",
    "        \"\"\"Enhanced features for larger grids\"\"\"\n",
    "        i, j = state\n",
    "        features = [\n",
    "            i / (self.size-1),  # Normalized x\n",
    "            j / (self.size-1),  # Normalized y\n",
    "             i / (self.size-1)*j / (self.size-1),  \n",
    "            (self.size-1 - i) / (self.size-1),  # Progress right\n",
    "            (self.size-1 - j) / (self.size-1),  # Progress down\n",
    "            abs(i - self.size//2) / self.size,  # Pit x-distance\n",
    "            abs(j - self.size//2) / self.size,  # Pit y-distance\n",
    "            float(i in [0, self.size-1]),  # Edge x\n",
    "            float(j in [0, self.size-1])   # Edge y\n",
    "        ]\n",
    "        return torch.FloatTensor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3c70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The structure of the policy (target) network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# The class of the deep q-learning agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=0.005, gamma=0.99):\n",
    "        self.policy_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    # Update weights and biases of policy network by batch\n",
    "    def update(self, batch):\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # Current Q values for chosen actions\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(-1))\n",
    "\n",
    "        # Next Q values from target network (using max)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0].unsqueeze(-1)\n",
    "            targets = rewards.unsqueeze(-1) + self.gamma * next_q * (1 - dones.unsqueeze(-1))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "# epsilon-greedy action selection implementation\n",
    "def epsilon_greedy(net, state, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim-1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return net(state.unsqueeze(0)).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a787364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the policy and target networks by q-targets\n",
    "def train_dqn(env, episodes=2000, batch_size=32, gamma=0.99):\n",
    "    state_dim = len(env._state_to_features((0,0)))\n",
    "    action_dim = len(env.actions)\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    buffer = deque(maxlen=10000) #replay buffer\n",
    "    epsilon = 1.0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = epsilon_greedy(agent.policy_net, state, epsilon, action_dim)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store experience in buffer (no next_action needed for Q-learning)\n",
    "            buffer.append((\n",
    "                state, \n",
    "                torch.LongTensor([action]), \n",
    "                torch.FloatTensor([reward]), \n",
    "                next_state, \n",
    "                torch.FloatTensor([done])\n",
    "            ))\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Train if enough samples in buffer\n",
    "            if len(buffer) >= batch_size:\n",
    "                batch_samples = random.sample(buffer, batch_size)\n",
    "                batch = [\n",
    "                    torch.stack([t[0] for t in batch_samples]),  # states\n",
    "                    torch.cat([t[1] for t in batch_samples]),    # actions\n",
    "                    torch.cat([t[2] for t in batch_samples]),    # rewards\n",
    "                    torch.stack([t[3] for t in batch_samples]),  # next_states\n",
    "                    torch.cat([t[4] for t in batch_samples])     # dones\n",
    "                ]\n",
    "                agent.update(batch)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % 10 == 0:\n",
    "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon=max(.01,1-episode/episodes)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1f7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -15, Epsilon: 1.00\n",
      "Episode 50, Reward: -46, Epsilon: 0.99\n",
      "Episode 100, Reward: -19, Epsilon: 0.97\n",
      "Episode 150, Reward: -35, Epsilon: 0.96\n",
      "Episode 200, Reward: -55, Epsilon: 0.95\n",
      "Episode 250, Reward: -26, Epsilon: 0.94\n",
      "Episode 300, Reward: -37, Epsilon: 0.93\n",
      "Episode 350, Reward: -17, Epsilon: 0.91\n",
      "Episode 400, Reward: -15, Epsilon: 0.90\n",
      "Episode 450, Reward: -26, Epsilon: 0.89\n",
      "Episode 500, Reward: -23, Epsilon: 0.88\n",
      "Episode 550, Reward: -43, Epsilon: 0.86\n",
      "Episode 600, Reward: -16, Epsilon: 0.85\n",
      "Episode 650, Reward: -40, Epsilon: 0.84\n",
      "Episode 700, Reward: -32, Epsilon: 0.82\n",
      "Episode 750, Reward: -6, Epsilon: 0.81\n",
      "Episode 800, Reward: -21, Epsilon: 0.80\n",
      "Episode 850, Reward: -13, Epsilon: 0.79\n",
      "Episode 900, Reward: -25, Epsilon: 0.78\n",
      "Episode 950, Reward: -15, Epsilon: 0.76\n",
      "Episode 1000, Reward: -6, Epsilon: 0.75\n",
      "Episode 1050, Reward: -25, Epsilon: 0.74\n",
      "Episode 1100, Reward: -13, Epsilon: 0.72\n",
      "Episode 1150, Reward: -17, Epsilon: 0.71\n",
      "Episode 1200, Reward: -19, Epsilon: 0.70\n",
      "Episode 1250, Reward: -26, Epsilon: 0.69\n",
      "Episode 1300, Reward: -3, Epsilon: 0.68\n",
      "Episode 1350, Reward: -18, Epsilon: 0.66\n",
      "Episode 1400, Reward: -14, Epsilon: 0.65\n",
      "Episode 1450, Reward: -17, Epsilon: 0.64\n",
      "Episode 1500, Reward: 0, Epsilon: 0.62\n",
      "Episode 1550, Reward: -10, Epsilon: 0.61\n",
      "Episode 1600, Reward: -11, Epsilon: 0.60\n",
      "Episode 1650, Reward: -16, Epsilon: 0.59\n",
      "Episode 1700, Reward: -20, Epsilon: 0.57\n",
      "Episode 1750, Reward: -3, Epsilon: 0.56\n",
      "Episode 1800, Reward: -7, Epsilon: 0.55\n",
      "Episode 1850, Reward: -19, Epsilon: 0.54\n",
      "Episode 1900, Reward: 1, Epsilon: 0.53\n",
      "Episode 1950, Reward: -8, Epsilon: 0.51\n",
      "Episode 2000, Reward: -28, Epsilon: 0.50\n",
      "Episode 2050, Reward: -7, Epsilon: 0.49\n",
      "Episode 2100, Reward: -8, Epsilon: 0.47\n",
      "Episode 2150, Reward: -14, Epsilon: 0.46\n",
      "Episode 2200, Reward: -9, Epsilon: 0.45\n",
      "Episode 2250, Reward: -4, Epsilon: 0.44\n",
      "Episode 2300, Reward: -5, Epsilon: 0.43\n",
      "Episode 2350, Reward: -8, Epsilon: 0.41\n",
      "Episode 2400, Reward: -4, Epsilon: 0.40\n",
      "Episode 2450, Reward: -2, Epsilon: 0.39\n",
      "Episode 2500, Reward: -3, Epsilon: 0.38\n",
      "Episode 2550, Reward: -3, Epsilon: 0.36\n",
      "Episode 2600, Reward: 1, Epsilon: 0.35\n",
      "Episode 2650, Reward: -4, Epsilon: 0.34\n",
      "Episode 2700, Reward: -1, Epsilon: 0.32\n",
      "Episode 2750, Reward: -15, Epsilon: 0.31\n",
      "Episode 2800, Reward: -15, Epsilon: 0.30\n",
      "Episode 2850, Reward: 3, Epsilon: 0.29\n",
      "Episode 2900, Reward: -1, Epsilon: 0.28\n",
      "Episode 2950, Reward: -1, Epsilon: 0.26\n",
      "Episode 3000, Reward: 1, Epsilon: 0.25\n",
      "Episode 3050, Reward: -3, Epsilon: 0.24\n",
      "Episode 3100, Reward: -9, Epsilon: 0.22\n",
      "Episode 3150, Reward: 0, Epsilon: 0.21\n",
      "Episode 3200, Reward: -5, Epsilon: 0.20\n",
      "Episode 3250, Reward: -23, Epsilon: 0.19\n",
      "Episode 3300, Reward: -3, Epsilon: 0.18\n",
      "Episode 3350, Reward: 3, Epsilon: 0.16\n",
      "Episode 3400, Reward: 1, Epsilon: 0.15\n",
      "Episode 3450, Reward: -1, Epsilon: 0.14\n",
      "Episode 3500, Reward: 1, Epsilon: 0.12\n",
      "Episode 3550, Reward: 1, Epsilon: 0.11\n",
      "Episode 3600, Reward: -16, Epsilon: 0.10\n",
      "Episode 3650, Reward: 2, Epsilon: 0.09\n",
      "Episode 3700, Reward: 1, Epsilon: 0.07\n",
      "Episode 3750, Reward: 1, Epsilon: 0.06\n",
      "Episode 3800, Reward: 2, Epsilon: 0.05\n",
      "Episode 3850, Reward: 3, Epsilon: 0.04\n",
      "Episode 3900, Reward: 1, Epsilon: 0.03\n",
      "Episode 3950, Reward: 3, Epsilon: 0.01\n",
      "\n",
      "Testing trained policy:\n",
      "Step 1: At (0, 0), took right, Reward: -1\n",
      "Step 2: At (0, 1), took right, Reward: -1\n",
      "Step 3: At (0, 2), took right, Reward: -1\n",
      "Step 4: At (0, 3), took right, Reward: -1\n",
      "Step 5: At (1, 3), took down, Reward: -1\n",
      "Step 6: At (1, 4), took right, Reward: -1\n",
      "Step 7: At (2, 4), took down, Reward: -1\n",
      "Step 8: At (3, 4), took down, Reward: -1\n",
      "Step 9: At (3, 3), took down, Reward: -1\n",
      "Step 10: At (3, 4), took right, Reward: -1\n",
      "Step 11: At (4, 4), took down, Reward: 10\n",
      "\n",
      "Total reward: 0\n",
      "Reached goal in 11 steps\n"
     ]
    }
   ],
   "source": [
    "# Test the learnt policy with greedy action selection\n",
    "def test_policy(env, agent):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    print(\"\\nTesting trained policy:\")\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = agent.policy_net(state.unsqueeze(0)).argmax().item()\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        print(f\"Step {steps}: At {env.current_state}, took {env.actions[action]}, Reward: {reward}\")\n",
    "    \n",
    "    print(f\"\\nTotal reward: {total_reward}\")\n",
    "    print(f\"Reached goal in {steps} steps\")\n",
    "\n",
    "# The main function to run\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld(size=5)\n",
    "    trained_agent = train_dqn(env, episodes=4000)\n",
    "    test_policy(env, trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7ebd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state(0,0): right,state(0,1): right,state(0,2): right,state(0,3): down,state(0,4): down,\n",
      "state(1,0): down,state(1,1): right,state(1,2): right,state(1,3): right,state(1,4): down,\n",
      "state(2,0): down,state(2,1): down,state(2,2): up,state(2,3): right,state(2,4): down,\n",
      "state(3,0): right,state(3,1): right,state(3,2): down,state(3,3): right,state(3,4): down,\n",
      "state(4,0): right,state(4,1): right,state(4,2): right,state(4,3): right,state(4,4): down,\n"
     ]
    }
   ],
   "source": [
    "# The greedy policy learnt for each state of the environment. \n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state=env._state_to_features((i,j))\n",
    "        with torch.no_grad():\n",
    "            action=trained_agent.policy_net(state.unsqueeze(0)).argmax().item()\n",
    "        print(f'state({i},{j}): {env.actions[action]}',end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e50310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d95b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
