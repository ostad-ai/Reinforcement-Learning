{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a9125a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Policy iteration\n",
    "\n",
    "The **Policy Iteration** is another model-based method used in **Reinforcement Learning** (RL) to obtain the optimal policy for a **Markov Decision Process** (MDP). The algorithm starts with a random policy. Then, it evaluates its value function called **policy evaluation**. After that, we improve the policy greedily called **policy improvement**. The process is repeated until convergence to the optimal policy. \n",
    "\n",
    "<br>The key steps in **Policy Iteration**:\n",
    "1. **Initialize** a random policy $\\pi$. Start with an arbitrary value function $v(s)$ for all states $s$.\n",
    "2. Repeat until policy converges:\n",
    "<br>a. **Policy Evaluation**:  \n",
    "<br>Compute $v_{\\pi(s)}$ for all states using iterative Bellman updates until it converges:\n",
    "<br>$v_{\\pi}(s)=\\sum_{s′} p(s′|s,\\pi(s))[r(s,\\pi(s),s′)+\\gamma v_{\\pi}(s′)]$\n",
    "<br>b. **Policy Improvement**:  \n",
    "<br>Update $\\pi(s) = argmax_a \\sum_{s'} p(s'|s,a) [r(s,a,s') + \\gamma v_{\\pi}(s')]$.  \n",
    "If policy $\\pi$ does not change, stop.  \n",
    "3. Return optimal $v$ and $\\pi$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the example in this Notebook, we use a **Grid World** environment (similar to the one we used in value iteration):\n",
    " - **States:** A 3x3 grid (9 states), labeled as (0,0) to (2,2).\n",
    " - **Actions:** Up, Down, Left, Right.\n",
    " - **Rewards:**\n",
    "    - Reaching the goal state (2,2) gives a reward of +10.\n",
    "    - Reaching a \"pit\" state (1,1) gives a reward of −10.\n",
    "    - All other transitions give a reward of −1.\n",
    "- **Terminal States:** (2,2) (goal) and (1,1) (pit).\n",
    "- **Transition Probabilities:**\n",
    "    - Moving in the intended direction succeeds with probability 0.8.\n",
    "    - With probability 0.2, the agent moves in a random direction\n",
    "\n",
    "<hr>\n",
    "https://github.com/ostad-ai/Reinforcement-Learning\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Reinforcement-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "645d074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define the grid world\n",
    "states = [(i, j) for i in range(3) for j in range(3)]  # 3x3 grid\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]              # Possible actions\n",
    "gamma = 0.9                                           # Discount factor\n",
    "theta = 1e-6                                          # Convergence threshold\n",
    "\n",
    "# Terminal states\n",
    "terminal_states = {(2, 2): 10, (1, 1): -10}  # (state: reward)\n",
    "\n",
    "# Transition probabilities\n",
    "def transition_probability(s, a, s_):\n",
    "    if s in terminal_states:  # Terminal states have no transitions\n",
    "        return 0\n",
    "    intended_s = get_intended_state(s, a)\n",
    "    if s_ == intended_s:\n",
    "        return 0.8\n",
    "    elif s_ in get_neighbors(s):\n",
    "        return 0.2 / len(get_neighbors(s))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Reward function\n",
    "def reward(s, a, s_):\n",
    "    if s_ in terminal_states:\n",
    "        return terminal_states[s_]\n",
    "    return -1  # Default reward for non-terminal transitions\n",
    "\n",
    "# The state the agent goes to, \n",
    "# from current state s, by taking action a\n",
    "# if everything was deterministic\n",
    "def get_intended_state(s, a):\n",
    "    i, j = s\n",
    "    if a == \"up\":\n",
    "        return (max(i - 1, 0), j)\n",
    "    elif a == \"down\":\n",
    "        return (min(i + 1, 2), j)\n",
    "    elif a == \"left\":\n",
    "        return (i, max(j - 1, 0))\n",
    "    elif a == \"right\":\n",
    "        return (i, min(j + 1, 2))\n",
    "\n",
    "# Immediate neighbors of the current state s \n",
    "def get_neighbors(s):\n",
    "    i, j = s\n",
    "    neighbors = []\n",
    "    for di, dj in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < 3 and 0 <= nj < 3:\n",
    "            neighbors.append((ni, nj))\n",
    "    return neighbors\n",
    "\n",
    "def policy_iteration(states, actions, P, R, gamma=0.9, theta=1e-6):\n",
    "    # 1. Initialize random policy\n",
    "    policy = {s: random.choice(actions) for s in states}\n",
    "    v = {s: 0 for s in states}\n",
    "    \n",
    "    while True:\n",
    "        # 2. Policy Evaluation (Iterative Bellman updates for fixed policy)\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in states:\n",
    "                if s in terminal_states:  # Skip terminal states\n",
    "                    continue\n",
    "                v_current = v[s]\n",
    "                a = policy[s]\n",
    "                v[s] = sum(P(s, a, s_) * (R(s, a, s_) + gamma * v[s_])\n",
    "                       for s_ in states)\n",
    "                delta = max(delta, abs(v_current - v[s]))\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        # 3. Policy Improvement (Greedy update)\n",
    "        policy_stable = True\n",
    "        for s in states:\n",
    "            if s in terminal_states:  # Skip terminal states\n",
    "                policy[s]=None\n",
    "                continue\n",
    "            old_action = policy[s]\n",
    "            policy[s] = max(actions, \n",
    "            key=lambda a: sum(P(s, a, s_) * (R(s, a, s_) + gamma * v[s_]) \n",
    "                                          for s_ in states))\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            return v, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf0a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: A 3*3 Grid World\n",
      "Algorithm: Policy Iteration\n",
      "------------------------------\n",
      "Optimal Value Function v(s):\n",
      "State (0, 0): 0.63, State (0, 1): 1.89, State (0, 2): 4.71, \n",
      "State (1, 0): 1.89, State (1, 1): 0.00, State (1, 2): 7.55, \n",
      "State (2, 0): 4.71, State (2, 1): 7.55, State (2, 2): 0.00, \n",
      "\n",
      "Optimal Policy π(s):\n",
      "State (0, 0): right, State (0, 1): right, State (0, 2): down, \n",
      "State (1, 0): down, State (1, 1): None, State (1, 2): down, \n",
      "State (2, 0): right, State (2, 1): right, State (2, 2): None, \n"
     ]
    }
   ],
   "source": [
    "# Run policy iteration\n",
    "v,optimal_policy=policy_iteration(states,actions,\n",
    "                                transition_probability,reward)\n",
    "print('Environment: A 3*3 Grid World')\n",
    "print('Algorithm: Policy Iteration')\n",
    "print(30*'-')\n",
    "print(\"Optimal Value Function v(s):\")\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        print(f\"State {(row,col)}: {v[(row,col)]:.2f}\",end=', ')\n",
    "    print('')\n",
    "print(\"\\nOptimal Policy \\u03c0(s):\")\n",
    "for row in range(3):\n",
    "    for col in range(3):\n",
    "        print(f\"State {(row,col)}: {optimal_policy[(row,col)]}\",\n",
    "              end=', ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fba45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
